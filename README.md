# Generating news tips for local journalism with topic modeling

This repository contains code for "Generating news tips for local journalism with topic modeling", a final project for Stanford's CS 230: Deep Learning class in Fall 2020.

The directory `\lda` contains:

* `build_data.py`: Inputs meeting agendas and meeting minutes as raw text files from `\data`, processes the data and outputs three csv files containing processed data: `train_results_200.csv`, `dev_results_200.csv` and `test_results.csv`. Usage: Run as a script from the command line, no flags needed.
* `lda.py`: Tunes hyperparameters one at a time by training the model based on command line flags. Inputs processed data in the form of a csv file and `params.json` files, outputs a log containing predicted topics, csv files recording training results as well as how the model performs on the dev set and  .jl files so that each model can be reloaded later. Usage: Run from the command line with flag indicating which hyperparameter to tune, where to read in/write out data and whether to run the script in test mode or train mode. Type `python lda.py --help` for details.
* `models`: Contains `utils.py` code from CS 230 example project, which is used to feed data from `params.json` files into `lda.py`.
* `\data`: Contains raw and processed data were kept locally. Because of space limitations on GitHub, I am only sharing a few sample files showing raw and processed files, rather than the full data. `build_data.py` reads in data from `\raw` and and writes data to `\processed`. `lda.py` reads data from `\processed`.
* `\experiments`: Contains sub-directories named by hyperparameter. Each hyperparameter sub-directory, in turn, contains a file `params.json` which acts as a template for experiments. `lda.py` writes `train_results_200.csv`, `dev_results_200.csv` and `test.csv` files as well as log files to these sub-directories. Because log files can be lengthy, only one log file, which was generated by the final test run of `lda.py` is included in `\experiments\num_topics` as an example.

The directory `\topicvec` contains scripts and associated outputs excerpted and adapted from TopicVec, the source code for "Generative Topic Embedding: a Continuous Representation of Documents" (ACL 2016) by Shaohua Li, Tat-Seng Chua, Jun Zhu and Chunyan Miao. The full implementation of TopicVec is here: https://github.com/askerlee/topicvec

* `csv2topic.py`: A script adapted from Li 2016. Inputs: unigram text file and word embeddings that ship with the original TopicVec implementation, csv files processed by `build_data.py`. Outputs: Log containing topics, csv files containing coherence scores evaluated on training set and unseen documents. This script is currently implemented to run in training mode. To run in test mode, edit the config dictionary with desired model inputs and set test = True.
* `topicvecDir.py`: A script adapted from Li 2016. Trains the TopicVec model using parameters specified in `csv2topic.py`. Performs EM iterations and inference.
* `utils.py`: A script adapted from Li 2016 with utility functions used in `csv2topic.py` and `topicvecDir.py`.
* `\data`: A directory containing processed data. In addition to the processed input text data from `build_data.py`, locally this directory includes the original vectors and unigram file provided in Li 2016. These vectors are not reposted here to save space.
* `\experiments`: A directory containing sub-directories for each hyperparameter to tune. 
* `test-11.16.log`: An example log demonstrating the output of `csv2topic.py` on the test set.
* `test-em100-best.topic.vec`, `test-em100-last.topic.vec`: Topic embeddings -- example output of csv2topic.py.

## Usage

Clone repository, then type:

`cd cs-230-project`
`pipenv install`
`pipenv shell`

## Requirements

* See Pipfile
* spacy's English language model: 'en_core_web_sm'
